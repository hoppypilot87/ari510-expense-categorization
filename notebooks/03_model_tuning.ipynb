{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1882840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook ready ✅\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 03_model_tuning.ipynb\n",
    "# Purpose: Hyperparameter tuning and cross-validation\n",
    "# ===============================================================\n",
    "print(\"Notebook ready ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19881ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# safe to re-run\n",
    "%pip install -q ipywidgets tqdm tqdm-joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b31e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 03_model_tuning.ipynb\n",
    "# Purpose: Hyperparameter tuning (CV) for Savings regression\n",
    "# ================================================================\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Distributions\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Cross-validation setup\n",
    "CV_FOLDS = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Explicit RMSE scorer (safe across sklearn versions)\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda yt, yp: -float(np.sqrt(mean_squared_error(yt, yp))),\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "def eval_regression(est, X_test, y_test):\n",
    "    \"\"\"Compute evaluation metrics for a fitted regression estimator.\"\"\"\n",
    "    y_pred = est.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = float(np.mean((y_test - y_pred) ** 2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return mae, mse, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3caf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Savings shapes: (124751, 17) (31188, 17)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Load processed dataset\n",
    "# ================================================================\n",
    "DATA_PATH = \"../data/processed/transactions_long.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "target_col = \"Desired_Savings\"\n",
    "drop_cols = [\"entity_id\", \"category\", \"Occupation\", \"City_Tier\", target_col]\n",
    "num_cols = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\").select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "X_savings = df[num_cols].copy()\n",
    "y_savings = df[target_col].astype(float).copy()\n",
    "\n",
    "# Train/test split\n",
    "X_savings_train, X_savings_test, y_savings_train, y_savings_test = train_test_split(\n",
    "    X_savings, y_savings, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Savings shapes:\", X_savings_train.shape, X_savings_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92598854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Tuning setup (run once, above 5A) ===\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "# Repro + CV config\n",
    "SEED      = 42\n",
    "CV_FOLDS  = 5            # k-folds\n",
    "N_ITER_REG = 24          # param samples per model (adjust to speed up/slow down)\n",
    "N_JOBS     = -1          # use all cores\n",
    "\n",
    "# Pipelines / estimators\n",
    "svr_pipe = make_pipeline(\n",
    "    # scale for SVR\n",
    "    StandardScaler(),\n",
    "    LinearSVR(dual=True,  # default True; good when n_samples > n_features\n",
    "              tol=1e-3,\n",
    "              max_iter=5000,\n",
    "              random_state=SEED)\n",
    ")\n",
    "\n",
    "rf_reg = RandomForestRegressor(\n",
    "    random_state=SEED,\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "# Parameter distributions for RandomizedSearchCV\n",
    "# (SVR): search C and epsilon on log scales; keep loss=\"epsilon_insensitive\" default\n",
    "svr_dist = {\n",
    "    \"linearsvr__C\":       loguniform(1e-2, 1e2),\n",
    "    \"linearsvr__epsilon\": loguniform(1e-3, 1.0),\n",
    "}\n",
    "\n",
    "# (RandomForest): moderate ranges to keep runtime reasonable\n",
    "rf_dist = {\n",
    "    \"n_estimators\":   randint(50, 200),\n",
    "    \"max_depth\":      randint(6, 18),\n",
    "    \"min_samples_split\": randint(2, 8),\n",
    "    \"min_samples_leaf\":  randint(1, 6),\n",
    "    \"max_features\":      [\"sqrt\", \"log2\"],\n",
    "    \"bootstrap\":         [True],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74799afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "► Running CV for regression: SVR (Savings)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091e3596cb9f4e379f51714ea0f37f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SVR (Savings) tuning:   0%|          | 0/36 [00:00<?, ?fits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV (neg RMSE): -2868.0748 | Params: {'linearsvr__epsilon': np.float64(0.3897685379286406), 'linearsvr__C': np.float64(36.06389385521764)}\n",
      "Fit time: 0.07s\n",
      "\n",
      "► Running CV for regression: RandomForest (Savings)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43405ff2609342e2a60298d8d56a4008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RandomForest (Savings) tuning:   0%|          | 0/180 [00:00<?, ?fits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === 5A. Run grid/random searches (REGRESSION: Savings) ======================\n",
    "# Fast knobs + progress bar CV search for SVR and RandomForest\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import (\n",
    "    KFold, cross_val_score, ParameterSampler\n",
    ")\n",
    "\n",
    "# ---------- tuning knobs (fast) ----------\n",
    "SEED       = 42\n",
    "N_JOBS     = -1\n",
    "CV_FOLDS   = 3         # 3-fold during tuning; keep 5-fold for final checks\n",
    "N_ITER_REG = 12        # #param samples per model (fast but useful)\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "cv  = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "scoring = 'neg_root_mean_squared_error'  # RMSE (negative)\n",
    "\n",
    "# ---------- subsample features for tuning only ----------\n",
    "def maybe_subsample(X, y, frac=0.35, random_state=SEED):\n",
    "    n = int(len(y) * frac)\n",
    "    idx = rng.choice(len(y), n, replace=False)\n",
    "    return X.iloc[idx] if hasattr(X, \"iloc\") else X[idx], y.iloc[idx] if hasattr(y, \"iloc\") else y[idx]\n",
    "\n",
    "X_savings_train_t, y_savings_train_t = maybe_subsample(X_savings_train, y_savings_train, frac=0.35, random_state=SEED)\n",
    "\n",
    "# ---------- estimators + distributions (narrower = faster & stabler) ----------\n",
    "svr_pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearSVR(dual=True, tol=1e-3, max_iter=5000, random_state=SEED)\n",
    ")\n",
    "svr_dist = {\n",
    "    \"linearsvr__C\":       np.exp(rng.uniform(np.log(1e-2), np.log(1e2), size=N_ITER_REG*2)),  # pre-sampled log-uniform\n",
    "    \"linearsvr__epsilon\": np.exp(rng.uniform(np.log(5e-2), np.log(5e-1), size=N_ITER_REG*2)),\n",
    "}\n",
    "\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=SEED\n",
    ")\n",
    "rf_dist = {\n",
    "    \"n_estimators\":      rng.randint(60, 141, size=N_ITER_REG*2),  # 60–140\n",
    "    \"max_depth\":         rng.randint(6, 15,  size=N_ITER_REG*2),   # 6–14\n",
    "    \"min_samples_split\": rng.randint(2, 7,   size=N_ITER_REG*2),   # 2–6\n",
    "    \"min_samples_leaf\":  rng.randint(1, 5,   size=N_ITER_REG*2),   # 1–4\n",
    "    \"max_features\":      [\"sqrt\"],                                  # drop \"log2\" to prune space\n",
    "    \"bootstrap\":         [True],\n",
    "}\n",
    "\n",
    "def param_sampler(dist, n_iter, rng):\n",
    "    \"\"\"\n",
    "    Turn our arrays/lists into a ParameterSampler-friendly dict.\n",
    "    If a value is an array/list, sample uniformly; if a scalar, use as-is.\n",
    "    \"\"\"\n",
    "    space = {}\n",
    "    for k, v in dist.items():\n",
    "        if isinstance(v, (list, np.ndarray)):\n",
    "            space[k] = v\n",
    "        else:\n",
    "            space[k] = [v]\n",
    "    return ParameterSampler(space, n_iter=n_iter, random_state=rng)\n",
    "\n",
    "def manual_cv_search(label, base_est, dist, X, y, n_iter=N_ITER_REG):\n",
    "    \"\"\"\n",
    "    Simple manual CV search with a tqdm progress bar.\n",
    "    Updates progress by CV_FOLDS per parameter set (so total = n_iter*CV_FOLDS).\n",
    "    Returns: best_estimator, best_cv_rmse, best_params, fit_time\n",
    "    \"\"\"\n",
    "    sampler = list(param_sampler(dist, n_iter, rng))\n",
    "    pbar = tqdm(total=len(sampler)*CV_FOLDS, desc=f\"{label} tuning\", unit=\"fits\")\n",
    "    best_rmse = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for params in sampler:\n",
    "        est = clone(base_est).set_params(**params)\n",
    "        # run k-fold CV (note: we update by CV_FOLDS for this set)\n",
    "        scores = cross_val_score(est, X, y, scoring=scoring, cv=cv, n_jobs=N_JOBS)\n",
    "        rmse = -np.mean(scores)\n",
    "        pbar.update(CV_FOLDS)\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse   = rmse\n",
    "            best_params = deepcopy(params)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Fit best on *full* (subsampled) training\n",
    "    best = clone(base_est).set_params(**best_params)\n",
    "    t0 = time.perf_counter()\n",
    "    best.fit(X, y)\n",
    "    fit_time = time.perf_counter() - t0\n",
    "\n",
    "    print(f\"Best CV (neg RMSE): {-best_rmse:.4f} | Params: {best_params}\")\n",
    "    print(f\"Fit time: {fit_time:.2f}s\")\n",
    "    return best, -best_rmse, best_params, fit_time  # return positive score for readability\n",
    "\n",
    "# ---------- run searches ----------\n",
    "print(\"\\n► Running CV for regression: SVR (Savings)\")\n",
    "best_svr, best_svr_score, best_svr_params, _ = manual_cv_search(\n",
    "    \"SVR (Savings)\", svr_pipe, svr_dist, X_savings_train_t, y_savings_train_t, n_iter=N_ITER_REG\n",
    ")\n",
    "\n",
    "print(\"\\n► Running CV for regression: RandomForest (Savings)\")\n",
    "best_rf, best_rf_score, best_rf_params, _ = manual_cv_search(\n",
    "    \"RandomForest (Savings)\", rf_reg, rf_dist, X_savings_train_t, y_savings_train_t, n_iter=N_ITER_REG*5  # a bit more budget\n",
    ")\n",
    "\n",
    "# ---------- evaluate on the held-out test set (full, not subsampled) ----------\n",
    "def evaluate_best(name, est, X_test, y_test):\n",
    "    from math import sqrt\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    y_pred = est.predict(X_test)\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    mse  = mean_squared_error(y_test, y_pred)\n",
    "    rmse = sqrt(mse)\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "    return {\"Model\": name, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "rows = []\n",
    "rows.append(evaluate_best(\"SVR (best)\",         best_svr, X_savings_test, y_savings_test))\n",
    "rows.append(evaluate_best(\"Random Forest (best)\", best_rf, X_savings_test, y_savings_test))\n",
    "\n",
    "reg_results_df = pd.DataFrame(rows).sort_values(\"RMSE\")\n",
    "print(\"\\n=== Savings → Regression (test set) ===\")\n",
    "display(reg_results_df)\n",
    "\n",
    "# keep for later summary cell if needed\n",
    "best_svr_savings     = best_svr\n",
    "best_svr_savings_cv  = best_svr_score\n",
    "best_rf_savings      = best_rf\n",
    "best_rf_savings_cv   = best_rf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5149ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLASSIFICATION: Expense Category Prediction (Multi-class)\n",
      "======================================================================\n",
      "\n",
      "Dataset for category classification:\n",
      "  Features (X): (155939, 18)\n",
      "  Target classes: 8 → ['Eating_Out', 'Education', 'Entertainment', 'Groceries', 'Healthcare', 'Miscellaneous', 'Transport', 'Utilities']\n",
      "  Class distribution:\n",
      "0    20000\n",
      "1    15939\n",
      "2    20000\n",
      "3    20000\n",
      "4    20000\n",
      "5    20000\n",
      "6    20000\n",
      "7    20000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train shape: (124751, 18) | Test shape: (31188, 18)\n",
      "\n",
      "► Tuning Logistic Regression (Category)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7521978ea01449f8fc57e7a4c666920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LogReg (Category) tuning:   0%|          | 0/45 [00:00<?, ?fits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best CV (f1_macro): 0.4890\n",
      "  Best params: {'solver': 'lbfgs', 'class_weight': None, 'C': np.float64(75.7947995334801)}\n",
      "  Fit time: 21.28s\n",
      "\n",
      "► Tuning Linear SVC (Category)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b852ced19d4e809c1fa67df9db7e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LinearSVC (Category) tuning:   0%|          | 0/45 [00:00<?, ?fits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 5B. Classification Model Tuning: Expense Category Prediction\n",
    "#     Build and tune classifiers to predict spending category\n",
    "# ================================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION: Expense Category Prediction (Multi-class)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ---------- Prepare classification dataset ----------\n",
    "# Load original data if not already loaded\n",
    "if \"df\" not in globals():\n",
    "    DATA_PATH = \"../data/processed/transactions_long.csv\"\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "TARGET_CAT = \"category\"\n",
    "ID_COLS = {\"entity_id\", \"txn_id\", \"index\", \"id\"}\n",
    "drop_for_clf = [c for c in df.columns if c in ID_COLS or (\"category_encoded\" in c)]\n",
    "\n",
    "# Extract features and target\n",
    "X_category = df.drop(columns=drop_for_clf, errors=\"ignore\").select_dtypes(include=[np.number]).copy()\n",
    "y_category_raw = df[TARGET_CAT].copy()\n",
    "\n",
    "# Encode category labels\n",
    "le_cat = LabelEncoder()\n",
    "y_category = le_cat.fit_transform(y_category_raw)\n",
    "\n",
    "print(f\"\\nDataset for category classification:\")\n",
    "print(f\"  Features (X): {X_category.shape}\")\n",
    "print(f\"  Target classes: {len(le_cat.classes_)} → {list(le_cat.classes_)}\")\n",
    "print(f\"  Class distribution:\\n{pd.Series(y_category, index=y_category_raw.index).value_counts().sort_index()}\\n\")\n",
    "\n",
    "# ---------- Train/test split with stratification ----------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_cat_train, X_cat_test, y_cat_train, y_cat_test = train_test_split(\n",
    "    X_category, y_category, test_size=0.2, random_state=SEED, stratify=y_category\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_cat_train.shape} | Test shape: {X_cat_test.shape}\")\n",
    "\n",
    "# ---------- Scale features for fair comparison ----------\n",
    "scaler_cat = StandardScaler()\n",
    "scaler_cat.fit(X_cat_train)\n",
    "X_cat_train_scaled = scaler_cat.transform(X_cat_train)\n",
    "X_cat_test_scaled = scaler_cat.transform(X_cat_test)\n",
    "\n",
    "# ---------- Tuning configuration ----------\n",
    "CV_FOLDS_CAT = 3          # 3-fold CV during tuning (fast)\n",
    "N_ITER_CLF = 15           # parameter samples per classifier\n",
    "N_JOBS = -1\n",
    "\n",
    "rng_cat = np.random.RandomState(SEED)\n",
    "cv_cat = StratifiedKFold(n_splits=CV_FOLDS_CAT, shuffle=True, random_state=SEED)\n",
    "scoring_clf = 'f1_macro'  # macro F1 for imbalanced multi-class\n",
    "\n",
    "# ---------- Estimators + parameter distributions ----------\n",
    "\n",
    "# 1. Logistic Regression (multinomial)\n",
    "log_clf = LogisticRegression(\n",
    "    max_iter=1000, multi_class='multinomial', \n",
    "    random_state=SEED, n_jobs=N_JOBS\n",
    ")\n",
    "log_dist = {\n",
    "    \"C\": np.exp(rng_cat.uniform(np.log(1e-2), np.log(1e2), size=N_ITER_CLF)),\n",
    "    \"solver\": [\"lbfgs\"],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "# 2. Linear SVC (one-vs-rest)\n",
    "svc_clf = LinearSVC(\n",
    "    dual=True, max_iter=2000, tol=1e-3,\n",
    "    random_state=SEED, class_weight='balanced'\n",
    ")\n",
    "svc_dist = {\n",
    "    \"C\": np.exp(rng_cat.uniform(np.log(1e-2), np.log(1e2), size=N_ITER_CLF)),\n",
    "}\n",
    "\n",
    "# 3. Random Forest (multi-class by default)\n",
    "rf_clf = RandomForestClassifier(\n",
    "    random_state=SEED, n_jobs=N_JOBS\n",
    ")\n",
    "rf_dist = {\n",
    "    \"n_estimators\": rng_cat.randint(50, 151, size=N_ITER_CLF),\n",
    "    \"max_depth\": rng_cat.randint(6, 16, size=N_ITER_CLF),\n",
    "    \"min_samples_split\": rng_cat.randint(2, 8, size=N_ITER_CLF),\n",
    "    \"min_samples_leaf\": rng_cat.randint(1, 5, size=N_ITER_CLF),\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "}\n",
    "\n",
    "# ---------- Helper: Manual CV search with progress bar ----------\n",
    "def param_sampler_helper(dist, n_iter, rng):\n",
    "    \"\"\"Convert distribution dict to ParameterSampler format.\"\"\"\n",
    "    from sklearn.model_selection import ParameterSampler\n",
    "    space = {}\n",
    "    for k, v in dist.items():\n",
    "        if isinstance(v, (list, np.ndarray)):\n",
    "            space[k] = v\n",
    "        else:\n",
    "            space[k] = [v]\n",
    "    return ParameterSampler(space, n_iter=n_iter, random_state=rng)\n",
    "\n",
    "def manual_cv_search_clf(label, base_est, dist, X, y, n_iter=N_ITER_CLF):\n",
    "    \"\"\"Manual CV search for classification with tqdm progress bar.\"\"\"\n",
    "    sampler = list(param_sampler_helper(dist, n_iter, rng_cat))\n",
    "    pbar = tqdm(total=len(sampler)*CV_FOLDS_CAT, desc=f\"{label} tuning\", unit=\"fits\")\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    for params in sampler:\n",
    "        est = clone(base_est).set_params(**params)\n",
    "        try:\n",
    "            # Use macro F1 for multi-class fairness\n",
    "            scores = cross_val_score(est, X, y, scoring=scoring_clf, cv=cv_cat, n_jobs=N_JOBS)\n",
    "            score = np.mean(scores)\n",
    "        except Exception as e:\n",
    "            score = -np.inf\n",
    "            \n",
    "        pbar.update(CV_FOLDS_CAT)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = deepcopy(params)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Fit best model on full training set\n",
    "    best = clone(base_est).set_params(**best_params)\n",
    "    t0 = time.perf_counter()\n",
    "    best.fit(X, y)\n",
    "    fit_time = time.perf_counter() - t0\n",
    "    \n",
    "    print(f\"  Best CV ({scoring_clf}): {best_score:.4f}\")\n",
    "    print(f\"  Best params: {best_params}\")\n",
    "    print(f\"  Fit time: {fit_time:.2f}s\")\n",
    "    \n",
    "    return best, best_score, best_params\n",
    "\n",
    "# ---------- Run classification tuning ----------\n",
    "print(\"\\n► Tuning Logistic Regression (Category)...\")\n",
    "best_log_clf, best_log_score, best_log_params = manual_cv_search_clf(\n",
    "    \"LogReg (Category)\", log_clf, log_dist, X_cat_train_scaled, y_cat_train, n_iter=N_ITER_CLF\n",
    ")\n",
    "\n",
    "print(\"\\n► Tuning Linear SVC (Category)...\")\n",
    "best_svc_clf, best_svc_score, best_svc_params = manual_cv_search_clf(\n",
    "    \"LinearSVC (Category)\", svc_clf, svc_dist, X_cat_train_scaled, y_cat_train, n_iter=N_ITER_CLF\n",
    ")\n",
    "\n",
    "print(\"\\n► Tuning Random Forest (Category)...\")\n",
    "best_rf_clf, best_rf_score, best_rf_params = manual_cv_search_clf(\n",
    "    \"RandomForest (Category)\", rf_clf, rf_dist, X_cat_train_scaled, y_cat_train, n_iter=N_ITER_CLF*2\n",
    ")\n",
    "\n",
    "# ---------- Evaluate on test set ----------\n",
    "def evaluate_classifier(name, est, X_test, y_test, y_test_raw=None):\n",
    "    \"\"\"Evaluate and return metrics for a classification model.\"\"\"\n",
    "    y_pred = est.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (macro)\": precision_macro,\n",
    "        \"Recall (macro)\": recall_macro,\n",
    "        \"F1 (macro)\": f1_macro,\n",
    "        \"F1 (weighted)\": f1_weighted,\n",
    "    }, y_pred\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION TEST SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "clf_results = []\n",
    "clf_predictions = {}\n",
    "\n",
    "result_log, pred_log = evaluate_classifier(\n",
    "    \"Logistic Regression (best)\", best_log_clf, X_cat_test_scaled, y_cat_test\n",
    ")\n",
    "clf_results.append(result_log)\n",
    "clf_predictions[\"LogReg\"] = pred_log\n",
    "\n",
    "result_svc, pred_svc = evaluate_classifier(\n",
    "    \"Linear SVC (best)\", best_svc_clf, X_cat_test_scaled, y_cat_test\n",
    ")\n",
    "clf_results.append(result_svc)\n",
    "clf_predictions[\"LinearSVC\"] = pred_svc\n",
    "\n",
    "result_rf, pred_rf = evaluate_classifier(\n",
    "    \"Random Forest (best)\", best_rf_clf, X_cat_test_scaled, y_cat_test\n",
    ")\n",
    "clf_results.append(result_rf)\n",
    "clf_predictions[\"RandomForest\"] = pred_rf\n",
    "\n",
    "# Display results\n",
    "clf_results_df = pd.DataFrame(clf_results).sort_values(\"F1 (macro)\", ascending=False)\n",
    "print(\"\\nCategory Classification Results (Test Set):\")\n",
    "display(clf_results_df)\n",
    "\n",
    "# Store best model for later use\n",
    "best_clf_model = best_rf_clf  # Default to RF; can be changed based on preference\n",
    "best_clf_name = \"Random Forest\"\n",
    "\n",
    "# ---------- Show detailed report for best model ----------\n",
    "print(f\"\\n{best_clf_name} - Detailed Classification Report:\")\n",
    "print(classification_report(y_cat_test, pred_rf, target_names=le_cat.classes_, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_cat_test, pred_rf)\n",
    "cm_df = pd.DataFrame(cm, index=[f\"True: {c}\" for c in le_cat.classes_],\n",
    "                      columns=[f\"Pred: {c}\" for c in le_cat.classes_])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "display(cm_df)\n",
    "\n",
    "print(\"\\n✅ Classification model tuning complete!\")\n",
    "print(f\"   Best model saved: best_rf_clf (Random Forest)\")\n",
    "print(f\"   Best CV score (macro F1): {best_rf_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
